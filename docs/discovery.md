# Discovery: Cloud Foundry KIND Deployment

**Date:** 2026-02-14
**Branch:** `feature/local-full-workloads`
**Upstream:** [cloudfoundry/kind-deployment](https://github.com/cloudfoundry/kind-deployment)

---

## What Upstream Deploys Today

The upstream `kind-deployment` repository deploys a near-complete Cloud Foundry
installation inside a KIND (Kubernetes in Docker) cluster using Helm charts
orchestrated by Helmfile.

### Infrastructure Components

| Component       | Chart Source       | Version    | Purpose                              |
| --------------- | ------------------ | ---------- | ------------------------------------ |
| Cilium          | helm.cilium.io     | 1.18.4     | CNI + kube-proxy replacement         |
| cert-manager    | jetstack           | v1.19.2    | TLS certificate management           |
| Istio Base      | istio              | 1.28.3     | Gateway API CRDs                     |
| Istiod          | istio              | 1.28.3     | Service mesh control plane           |
| PostgreSQL      | bitnami            | 16.7.27    | Shared relational database           |
| MinIO           | bitnami            | 17.0.21    | S3-compatible blobstore              |
| NATS            | nats-io            | 2.12.4     | Messaging bus                        |

### Cloud Foundry Components

| Component            | Source                     | Purpose                             |
| -------------------- | -------------------------- | ----------------------------------- |
| UAA                  | releases/uaa/helm          | Identity and OAuth2 provider        |
| CAPI (Cloud Controller) | releases/capi/helm      | CF API server                       |
| Diego (BBS, Auctioneer, File Server, SSH Proxy) | releases/diego/helm | Container orchestration |
| Locket               | releases/diego/helm        | Distributed locking                 |
| k8s-rep              | ghcr.io/cloudfoundry/helm  | v0.4.0 - K8s-native Diego rep       |
| Route Emitter        | releases/diego/helm        | Route registration with NATS        |
| Routing (Gorouter)   | releases/routing/helm      | HTTP request routing                |
| TPS Watcher          | releases/capi/helm         | Task/process status reporting       |
| Loggregator          | releases/loggregator/helm  | Log aggregation (optional)          |
| Loggregator Agent    | releases/loggregator-agent/helm | Log collection agents          |
| Log Cache            | releases/log-cache/helm    | Log storage and retrieval           |
| CredHub              | releases/credhub/helm      | Credential management               |
| CF Networking        | releases/cf-networking/helm | Container-to-container networking  |
| Policy Agent         | ghcr.io/cloudfoundry/helm  | v0.2.1 - Network policy enforcement |
| NFS Volume (optional)| releases/nfs-volume/helm   | NFS volume services                 |

### Rootfs

| Image       | Version | Source                                |
| ----------- | ------- | ------------------------------------- |
| cflinuxfs4  | 1.307.0 | ghcr.io/cloudfoundry/k8s/cflinuxfs4   |

### KIND Cluster Topology

The cluster is named `cfk8s` with 3 nodes:

1. **control-plane** -- Standard K8s control plane
2. **worker** -- Ingress node with NodePort mappings:
   - 31080 -> host port 80 (HTTP)
   - 31443 -> host port 443 (HTTPS)
   - 31222 -> host port 2222 (SSH)
3. **cell worker** -- Labeled `cloudfoundry.org/cell=true`, tainted with
   `cloudfoundry.org/cell=true:NoSchedule` so only CF workloads schedule here

### Networking

- **CNI:** Cilium replaces both the default CNI and kube-proxy
- **Ingress:** Istio Gateway API via NodePort services
- **DNS:** CoreDNS patched with a rewrite rule:
  `*.127-0-0-1.nip.io` -> `istio-gateway-istio.default.svc.cluster.local`
- **Domain:** `api.127-0-0-1.nip.io` (CF API), `*.apps.127-0-0-1.nip.io` (apps)
- **Container networking:** `apps.internal` forwarded to bosh-dns (when policy support enabled)

### Image Caching

Pull-through registry caches run as docker-compose services on the `kind` Docker
network:

- `docker-io` -- mirrors docker.io
- `ghcr-io` -- mirrors ghcr.io
- `quay-io` -- mirrors quay.io

Cached data persists across cluster restarts via Docker volumes.

### Secrets

All secrets are generated by `scripts/init.sh` into `temp/secrets.sh`:

- Database password
- Blobstore password
- OAuth client secret
- CC admin password
- UAA admin secret
- Diego SSH credentials
- SSH proxy key fingerprint

CA certificate and SSH keys are generated into `temp/certs/`.

### Feature Flags

Enabled by `scripts/set_feature_flags.sh` after bootstrap:

- `diego_cnb` -- Cloud Native Buildpack support
- `diego_docker` -- Docker image push support
- `service_instance_sharing` -- Cross-space service sharing
- `resource_matching` -- Disabled (not needed locally)

---

## Supported Features

The upstream documents support for nearly all CF features:

- `cf push` with classic buildpacks, CNB, and Docker images
- App SSH (`cf ssh`)
- Tasks (`cf run-task`)
- Container-to-container networking and service discovery (`apps.internal`)
- CredHub for credential management
- Log streaming (`cf logs`)
- Multiple buildpacks (Java, Node.js, Go, Binary; optionally all others)
- Health checks (http, port, process)
- App scaling
- Rolling deployments
- NFS volume services (optional, disabled by default)

---

## Gaps to Full Workloads

### Not Yet Supported (Upstream-Acknowledged)

| Gap               | Impact                                                  | Difficulty |
| ----------------- | ------------------------------------------------------- | ---------- |
| TCP Routing       | No TCP route mapping; apps requiring non-HTTP protocols  | High       |
| Routing API       | No route services, shared domains via API                | Medium     |

### Not Supported (Platform Limitations)

| Gap                    | Reason                                               |
| ---------------------- | ---------------------------------------------------- |
| Windows cells          | KIND runs Linux containers only                      |
| Security groups (ASGs) | Not implemented in k8s-rep; iptables rules N/A       |
| Isolation segments     | Single cell zone by default (z1); CI adds z2 for CATs |
| Route services         | Requires Routing API (not deployed)                  |
| WebSocket keepalive    | Gorouter config may need tuning for long-lived WS    |

### Partially Supported

| Feature              | Status                                                 |
| -------------------- | ------------------------------------------------------ |
| Multiple AZs         | CI workflow appends a z2 cell node for CATs testing    |
| Loggregator          | Enabled by default, can be disabled                    |
| Container networking | Policy server + agent deployed, bosh-dns patched       |

---

## Constraints / Resource Expectations

### Minimum Requirements

| Resource | Minimum   | Recommended | Notes                                   |
| -------- | --------- | ----------- | --------------------------------------- |
| RAM      | 8 GB      | 12-16 GB    | Docker Desktop allocation               |
| CPU      | 4 cores   | 8 cores     | Affects install time significantly       |
| Disk     | 10 GB     | 15-20 GB    | Images cached across runs               |
| Network  | Required  | Broadband   | First run downloads several GB of images |

### Timing Expectations

| Scenario                | Duration     |
| ----------------------- | ------------ |
| First `make up` (cold)  | 5-20 minutes |
| Subsequent `make up`    | 2-7 minutes  |
| `make bootstrap`        | 1-2 minutes  |
| `cf push` (hello-js)    | 1-3 minutes  |

### Platform Notes

- **macOS:** Docker Desktop with Apple Virtualization framework recommended.
  Enable Rosetta for x86_64/amd64 emulation on Apple Silicon.
- **Linux:** Docker Engine works directly. May need
  `fs.inotify.max_user_instances=512` (CI sets this).
- **Windows:** WSL2 + Docker Desktop. The `make` tool must be installed
  separately.

---

## Decision Log

### Why This Approach (KIND + Helm) vs Korifi

| Factor             | kind-deployment (this repo)         | Korifi                           |
| ------------------ | ----------------------------------- | -------------------------------- |
| Components         | Real CF components (Diego, CAPI, UAA) | Re-implemented in Go controllers |
| Compatibility      | Near-100% CF CLI compatibility      | Subset of CF API                 |
| Buildpacks         | Classic + CNB + Docker              | CNB only (via kpack)             |
| SSH                | Full `cf ssh` support               | Not supported                    |
| Tasks              | Full `cf run-task`                  | Supported                        |
| CredHub            | Deployed                            | Not available                    |
| Resource usage     | Higher (12-16 GB)                   | Lower (4-8 GB)                   |
| Installation       | `make up` (single command)          | Multiple kubectl/helm steps      |
| Target audience    | CF operators, full-fidelity testing | K8s-native CF API consumers      |

**Decision:** This fork extends `kind-deployment` because it deploys the real
CF components, providing full-fidelity testing for workloads that depend on
Diego, CredHub, SSH, classic buildpacks, and other features absent from Korifi.

### Why Helm + Helmfile (Not ytt/kapp)

The original `cf-for-k8s` project used ytt (YAML templating) and kapp
(deployment tool) from the Carvel toolchain. The upstream `kind-deployment`
chose Helm + Helmfile instead:

1. **Industry standard:** Helm is the dominant K8s package manager. Most
   developers already know it.
2. **GoTemplate values:** Standard Helm values files, no new templating language
   to learn.
3. **Dependency ordering:** Helmfile `needs:` provides explicit dependency DAGs
   between releases.
4. **No CF-specific tooling:** ytt/kapp require installing Carvel tools that are
   unfamiliar outside the CF ecosystem.
5. **Ecosystem integration:** Helm charts can be published to OCI registries,
   consumed by ArgoCD/Flux, and versioned with Renovate.

### Why nip.io for DNS

Using `*.127-0-0-1.nip.io` provides wildcard DNS resolution to `127.0.0.1`
without requiring `/etc/hosts` modifications or a local DNS server. The domain
resolves via public DNS, which means:

- No sudo required for DNS setup
- Works with any DNS resolver (with caveats for corporate DNS that blocks nip.io)
- Wildcard subdomains work automatically for app routes

### Why Pull-Through Caches

Docker Hub rate limits (100 pulls/6h for anonymous, 200 for authenticated) can
cause deployment failures. The pull-through caches:

- Eliminate rate limiting after first pull
- Speed up subsequent deployments significantly
- Cache images for docker.io, ghcr.io, and quay.io
- Persist across cluster recreations via Docker volumes

---

## CI Versions (Pinned)

These versions are used in upstream CI and represent the known-good baseline:

| Tool      | Version | Source                                  |
| --------- | ------- | --------------------------------------- |
| KIND      | 0.30.0  | kind.sigs.k8s.io                        |
| Helmfile  | 1.2.3   | github.com/helmfile/helmfile            |
| CF CLI    | 8.14.1  | packages.cloudfoundry.org               |
| Helm      | 3.x     | helm.sh (installed via helmfile plugin) |
